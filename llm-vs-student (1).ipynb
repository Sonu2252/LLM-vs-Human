{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":61542,"databundleVersionId":6888007,"sourceType":"competition"}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n\ndf = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:30:00.140050Z","iopub.execute_input":"2023-12-05T07:30:00.140598Z","iopub.status.idle":"2023-12-05T07:30:00.232726Z","shell.execute_reply.started":"2023-12-05T07:30:00.140560Z","shell.execute_reply":"2023-12-05T07:30:00.230945Z"},"trusted":true},"execution_count":115,"outputs":[{"execution_count":115,"output_type":"execute_result","data":{"text/plain":"         id  prompt_id                                               text  \\\n0  0059830c          0  Cars. Cars have been around since they became ...   \n1  005db917          0  Transportation is a large necessity in most co...   \n2  008f63e3          0  \"America's love affair with it's vehicles seem...   \n3  00940276          0  How often do you ride in a car? Do you drive a...   \n4  00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n\n   generated  \n0          0  \n1          0  \n2          0  \n3          0  \n4          0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt_id</th>\n      <th>text</th>\n      <th>generated</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0059830c</td>\n      <td>0</td>\n      <td>Cars. Cars have been around since they became ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>005db917</td>\n      <td>0</td>\n      <td>Transportation is a large necessity in most co...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>008f63e3</td>\n      <td>0</td>\n      <td>\"America's love affair with it's vehicles seem...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00940276</td>\n      <td>0</td>\n      <td>How often do you ride in a car? Do you drive a...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00c39458</td>\n      <td>0</td>\n      <td>Cars are a wonderful thing. They are perhaps o...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Generative Model:\n**An AI model that is trained to generate new data samples that tries to resemble given set of training samples. Genrative models are used for image generation, Natural Language Processing, data augmnetation and many more. \nHere, in Large Language Model System, we use the same principle to generate the text related to the training samples. The main downside of generative model is low variability distribution mean.\nFor this we exploit this low variability distribution mean and find out the difference between the AI generated text and student written text.\nA low variance will have values grouped around the mean (e.g. a narrow bell shape), whereas a high vaiance will have values spread out from the norm.**\n","metadata":{}},{"cell_type":"code","source":"from scipy.stats import wasserstein_distance ","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:30:00.236244Z","iopub.execute_input":"2023-12-05T07:30:00.236758Z","iopub.status.idle":"2023-12-05T07:30:00.244713Z","shell.execute_reply.started":"2023-12-05T07:30:00.236720Z","shell.execute_reply":"2023-12-05T07:30:00.242652Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"markdown","source":"# Wasserstein Distance\n\n**Here I need a probability distribution of words written which are first converted to word 2 vector representation then to a numpy array, with all the array sum to 0, then listing down the highest value of wassertein distance between them, and according to that formulating our context, whether the given thing is generated by LLM or written by a student.**\n**Wasserstein distance (also known as Earth Mover's Distance or Wasserstein metric) is a measure of the distance between two probability distributions over a metric space. It's used to quantify the amount of \"work\" needed to transform one distribution into the other. In the context of probability distributions, it is common to compute the Wasserstein distance between empirical distributions (histograms) or probability density functions.**\n\n**The Wasserstein distance is typically a non-negative real number. A higher Wasserstein distance indicates greater dissimilarity between the two distributions.**\n\n**High Wasserstein Distance: If the Wasserstein distance between two distributions is high, it suggests that the two distributions are significantly different. There is more \"work\" required to transform one into the other.**\n\n**Low Wasserstein Distance: If the Wasserstein distance is low, it suggests that the two distributions are more similar. The amount of \"work\" needed to transform one into the other is relatively small.**","metadata":{}},{"cell_type":"code","source":"df.value_counts('generated')","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:30:00.247211Z","iopub.execute_input":"2023-12-05T07:30:00.247870Z","iopub.status.idle":"2023-12-05T07:30:00.271928Z","shell.execute_reply.started":"2023-12-05T07:30:00.247815Z","shell.execute_reply":"2023-12-05T07:30:00.270836Z"},"trusted":true},"execution_count":117,"outputs":[{"execution_count":117,"output_type":"execute_result","data":{"text/plain":"generated\n0    1375\n1       3\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:30:00.274898Z","iopub.execute_input":"2023-12-05T07:30:00.275852Z","iopub.status.idle":"2023-12-05T07:30:00.294403Z","shell.execute_reply.started":"2023-12-05T07:30:00.275701Z","shell.execute_reply":"2023-12-05T07:30:00.292650Z"},"trusted":true},"execution_count":118,"outputs":[{"execution_count":118,"output_type":"execute_result","data":{"text/plain":"id           0\nprompt_id    0\ntext         0\ngenerated    0\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"# '0' generated by a student and '1' generated by a LLM","metadata":{}},{"cell_type":"code","source":"index_llm = df.loc[df['generated'] == 1].index.tolist()\nprint(index_llm)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:30:00.299695Z","iopub.execute_input":"2023-12-05T07:30:00.300761Z","iopub.status.idle":"2023-12-05T07:30:00.312069Z","shell.execute_reply.started":"2023-12-05T07:30:00.300701Z","shell.execute_reply":"2023-12-05T07:30:00.310588Z"},"trusted":true},"execution_count":119,"outputs":[{"name":"stdout","text":"[704, 740, 1262]\n","output_type":"stream"}]},{"cell_type":"code","source":"index_llm= [index for index in index_llm if index in df.index]\n\n# Drop rows by index if the indices exist\ndf1 = df.drop(index_llm, errors='ignore')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:30:00.313861Z","iopub.execute_input":"2023-12-05T07:30:00.314436Z","iopub.status.idle":"2023-12-05T07:30:00.328169Z","shell.execute_reply.started":"2023-12-05T07:30:00.314386Z","shell.execute_reply":"2023-12-05T07:30:00.326515Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"df1.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:30:00.330437Z","iopub.execute_input":"2023-12-05T07:30:00.331102Z","iopub.status.idle":"2023-12-05T07:30:00.350266Z","shell.execute_reply.started":"2023-12-05T07:30:00.331049Z","shell.execute_reply":"2023-12-05T07:30:00.349216Z"},"trusted":true},"execution_count":121,"outputs":[{"execution_count":121,"output_type":"execute_result","data":{"text/plain":"         id  prompt_id                                               text  \\\n0  0059830c          0  Cars. Cars have been around since they became ...   \n1  005db917          0  Transportation is a large necessity in most co...   \n2  008f63e3          0  \"America's love affair with it's vehicles seem...   \n3  00940276          0  How often do you ride in a car? Do you drive a...   \n4  00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n\n   generated  \n0          0  \n1          0  \n2          0  \n3          0  \n4          0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt_id</th>\n      <th>text</th>\n      <th>generated</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0059830c</td>\n      <td>0</td>\n      <td>Cars. Cars have been around since they became ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>005db917</td>\n      <td>0</td>\n      <td>Transportation is a large necessity in most co...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>008f63e3</td>\n      <td>0</td>\n      <td>\"America's love affair with it's vehicles seem...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00940276</td>\n      <td>0</td>\n      <td>How often do you ride in a car? Do you drive a...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00c39458</td>\n      <td>0</td>\n      <td>Cars are a wonderful thing. They are perhaps o...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"length = len(df1)\nprint(length)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:30:00.352117Z","iopub.execute_input":"2023-12-05T07:30:00.352894Z","iopub.status.idle":"2023-12-05T07:30:00.360585Z","shell.execute_reply.started":"2023-12-05T07:30:00.352852Z","shell.execute_reply":"2023-12-05T07:30:00.358803Z"},"trusted":true},"execution_count":122,"outputs":[{"name":"stdout","text":"1375\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Here we have very low samples for text generated by LLM, so rather than using some pre-trained model, let us dive deep into the way which generative model is tested from the real model","metadata":{}},{"cell_type":"code","source":"import nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nnltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:30:00.363006Z","iopub.execute_input":"2023-12-05T07:30:00.363494Z","iopub.status.idle":"2023-12-05T07:30:00.498503Z","shell.execute_reply.started":"2023-12-05T07:30:00.363450Z","shell.execute_reply":"2023-12-05T07:30:00.496954Z"},"trusted":true},"execution_count":123,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":123,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"#from nltk.stem.porter import PorterStemmer","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:30:00.500700Z","iopub.execute_input":"2023-12-05T07:30:00.501092Z","iopub.status.idle":"2023-12-05T07:30:00.507020Z","shell.execute_reply.started":"2023-12-05T07:30:00.501059Z","shell.execute_reply":"2023-12-05T07:30:00.505635Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"#ps = PorterStemmer()\nlist_essay = []\nfor i in range(length+3):\n    tr = re.sub('[^a-zA-Z]', ' ', df['text'][i])\n    tr = tr.lower()\n    tr = tr.split()\n    tr = [word for word in tr if word not in stopwords.words('english')]\n    tr = ' '.join(tr)\n    list_essay.append(tr)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:30:00.509263Z","iopub.execute_input":"2023-12-05T07:30:00.511076Z","iopub.status.idle":"2023-12-05T07:31:52.519006Z","shell.execute_reply.started":"2023-12-05T07:30:00.511021Z","shell.execute_reply":"2023-12-05T07:31:52.517788Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"len(list_essay)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:31:52.520432Z","iopub.execute_input":"2023-12-05T07:31:52.520813Z","iopub.status.idle":"2023-12-05T07:31:52.529469Z","shell.execute_reply.started":"2023-12-05T07:31:52.520782Z","shell.execute_reply":"2023-12-05T07:31:52.528201Z"},"trusted":true},"execution_count":126,"outputs":[{"execution_count":126,"output_type":"execute_result","data":{"text/plain":"1378"},"metadata":{}}]},{"cell_type":"code","source":"list_essay[2]","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:31:52.531179Z","iopub.execute_input":"2023-12-05T07:31:52.531714Z","iopub.status.idle":"2023-12-05T07:31:52.546144Z","shell.execute_reply.started":"2023-12-05T07:31:52.531683Z","shell.execute_reply":"2023-12-05T07:31:52.545033Z"},"trusted":true},"execution_count":127,"outputs":[{"execution_count":127,"output_type":"execute_result","data":{"text/plain":"'america love affair vehicles seems cooling says elisabeth rosenthal understand rosenthal perspective easier suggest america car usage decreasing slowly necessarily bad sense certain positive effects advantages limiting car usage includes increase security health along decrease pollution dependence firstly car usage limited security health likely guaranteed feeling secure highly important individuals everywhere example many people colombia used public transportation car free day leaving streets capital city according andrew selsky eerily devoid traffic jams complications stem traffic jams end feeling confidence plan get point b simple second ago complication personal plans leads become stressed feeling doubt overcomes thoughts car usage limited would control much traffic accumulates thus minimizing chance stress heidrun walter states car always tense much happier way car usage minimize conditions detrimental health also enlarges capacity exercise main purpose car get someone one place another important job takes personal life becomes difficult things enjoyed life limits car usage forces stay shape according andrew selsky parks sports centers also bloomed throughout city less cars means healthier natural situations parks sport centers becoming efficient becomes easier find physically active population overall less usage cars minimizes stress increases health secondly limting car usage becomes beneficial environment days people become annoyed others care passionately environment look behind constant cries action solid facts yespollution bad environment yes bad envorment means unhealthy living yes cars one main contributors pollution environment pattern less car usage elisabeth rosenthal states beneficial implications carbon emissions environment less use cars less pollution environment one must observe limiting car usage opportunity create cleaner world better future effects pollution environment completley dangerous car users blame additionally would lower dependence cars many people today find car useful many features form transportation many figure would possesion development people interaction technology left wide gap historic natural ways thought modern society dependent always good individuals david goldberg says development since world war ii centered car change many people could disagree wonder necessary change ways especially highly devloped developed means dependent harmful machine could effective devlopment according elisabeth rosenthal cashstrapped americans could afford new cars unemployed going work anyway many people precious luxury private transportation first place become distant natural society peope become use cars become oblivious significant effects limits car usage effcts could controlled conclude advantages limiting car usage increase health along decrease pollution less dependence cars limiting car usage positive way enfore organized clean environment ensure health security live one reason america reffered succesful country america decreased use vehicles fact done best majority'"},"metadata":{}}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:31:52.554112Z","iopub.execute_input":"2023-12-05T07:31:52.554761Z","iopub.status.idle":"2023-12-05T07:31:52.560051Z","shell.execute_reply.started":"2023-12-05T07:31:52.554725Z","shell.execute_reply":"2023-12-05T07:31:52.558794Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"import gensim\n","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:31:52.562006Z","iopub.execute_input":"2023-12-05T07:31:52.562467Z","iopub.status.idle":"2023-12-05T07:31:52.570361Z","shell.execute_reply.started":"2023-12-05T07:31:52.562424Z","shell.execute_reply":"2023-12-05T07:31:52.569213Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"from gensim.models import Word2Vec\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nimport nltk\nnltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:31:52.571991Z","iopub.execute_input":"2023-12-05T07:31:52.572318Z","iopub.status.idle":"2023-12-05T07:31:52.584050Z","shell.execute_reply.started":"2023-12-05T07:31:52.572291Z","shell.execute_reply":"2023-12-05T07:31:52.582966Z"},"trusted":true},"execution_count":130,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":130,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# Tokenize each word in the list\n#tokenized_words = [word_tokenize(sentence.lower()) for sentence in list_essay]\n\n# Flatten the list of lists into a single list\n#essay_llm_1 = [token for sublist in tokenized_words for token in sublist]\n#print(tokenized_words)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:31:52.585695Z","iopub.execute_input":"2023-12-05T07:31:52.586016Z","iopub.status.idle":"2023-12-05T07:31:52.594698Z","shell.execute_reply.started":"2023-12-05T07:31:52.585990Z","shell.execute_reply":"2023-12-05T07:31:52.593773Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"essay_stu_1 = list_essay[0]\nessay_llm_1 = list_essay[704]      #At 704 indexed list, the essay is written by LLM.\nessay_llm_2 = list_essay[740]\nessay_llm_3 = list_essay[1262]\nessay_stu_2 = list_essay[1]\nessay_stu_3 = list_essay[2]","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:31:52.595800Z","iopub.execute_input":"2023-12-05T07:31:52.596130Z","iopub.status.idle":"2023-12-05T07:31:52.607571Z","shell.execute_reply.started":"2023-12-05T07:31:52.596101Z","shell.execute_reply":"2023-12-05T07:31:52.606579Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"list_essay = [str(item) for item in list_essay]\n","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:31:52.609272Z","iopub.execute_input":"2023-12-05T07:31:52.609665Z","iopub.status.idle":"2023-12-05T07:31:52.621066Z","shell.execute_reply.started":"2023-12-05T07:31:52.609634Z","shell.execute_reply":"2023-12-05T07:31:52.619888Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"markdown","source":"# Here, I take vector size as 1 in word2vec.","metadata":{}},{"cell_type":"code","source":"from gensim.models import Word2Vec\nfrom nltk.tokenize import word_tokenize\n\n# Assuming list_essay is a list of sentences\ntokenized_sentences = [word_tokenize(sentence) for sentence in list_essay]\n\n# Define the Word2Vec model\nmodel = Word2Vec(sentences=tokenized_sentences, vector_size=1, window=5, min_count=1, workers=4)  # Adjust the parameters as needed\n\n# Training the model\nmodel.train(tokenized_sentences, total_examples=len(tokenized_sentences), epochs=10)\n\ndef tokenize_word_vec_1(essay):\n    tokenized_words = word_tokenize(essay)  # Use the same tokenizer\n    list_essay = []\n    \n    for word in tokenized_words:\n        try:\n            vector = model.wv[word]\n            list_essay.append(vector)\n        except KeyError:\n            # Handle the case where the word is not in the vocabulary\n            # You might want to ignore it, assign a special vector, or handle it differently based on your use case\n            pass\n    \n    float_list = [float(arr[0]) for arr in list_essay]\n    return float_list\n","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:31:52.622347Z","iopub.execute_input":"2023-12-05T07:31:52.623017Z","iopub.status.idle":"2023-12-05T07:32:01.684395Z","shell.execute_reply.started":"2023-12-05T07:31:52.622984Z","shell.execute_reply":"2023-12-05T07:32:01.683053Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"vec_llm1 = tokenize_word_vec_1(essay_llm_1)\nvec_llm2 = tokenize_word_vec_1(essay_llm_2)\nvec_llm3 = tokenize_word_vec_1(essay_llm_3)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:32:01.686022Z","iopub.execute_input":"2023-12-05T07:32:01.686676Z","iopub.status.idle":"2023-12-05T07:32:01.700715Z","shell.execute_reply.started":"2023-12-05T07:32:01.686636Z","shell.execute_reply":"2023-12-05T07:32:01.699135Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"\nlist_value_1 = []\nlist_value_2 = []\nlist_value_3 = []\ndistribution_p = np.array(vec_llm1) / np.sum(vec_llm1)\ndistribution_q = np.array(vec_llm2) / np.sum(vec_llm2)\ndistribution_r = np.array(vec_llm3) / np.sum(vec_llm3)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:32:01.702743Z","iopub.execute_input":"2023-12-05T07:32:01.703120Z","iopub.status.idle":"2023-12-05T07:32:01.716122Z","shell.execute_reply.started":"2023-12-05T07:32:01.703089Z","shell.execute_reply":"2023-12-05T07:32:01.715123Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"def apply_was_1(essay_stu_1, dist_p):\n    list_1 = []\n    vec_stu = tokenize_word_vec_1(essay_stu_1)\n    dist_a = np.array(vec_stu) / np.sum(vec_stu)\n    wass_val = wasserstein_distance(dist_a, dist_p)\n    list_1.append(wass_val)\n    return wass_val","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:32:01.717282Z","iopub.execute_input":"2023-12-05T07:32:01.718505Z","iopub.status.idle":"2023-12-05T07:32:01.735576Z","shell.execute_reply.started":"2023-12-05T07:32:01.718466Z","shell.execute_reply":"2023-12-05T07:32:01.734613Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"len(list_essay)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:32:01.736899Z","iopub.execute_input":"2023-12-05T07:32:01.737441Z","iopub.status.idle":"2023-12-05T07:32:01.750866Z","shell.execute_reply.started":"2023-12-05T07:32:01.737411Z","shell.execute_reply":"2023-12-05T07:32:01.749894Z"},"trusted":true},"execution_count":138,"outputs":[{"execution_count":138,"output_type":"execute_result","data":{"text/plain":"1378"},"metadata":{}}]},{"cell_type":"code","source":"for i in range(len(list_essay)):\n    value = apply_was_1(list_essay[i], distribution_p)\n    list_value_1.append(value)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:32:01.752659Z","iopub.execute_input":"2023-12-05T07:32:01.753337Z","iopub.status.idle":"2023-12-05T07:32:07.993006Z","shell.execute_reply.started":"2023-12-05T07:32:01.753294Z","shell.execute_reply":"2023-12-05T07:32:07.991190Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"for i in range(len(list_essay)):\n    value = apply_was_1(list_essay[i], distribution_q)\n    list_value_2.append(value)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:32:07.994672Z","iopub.execute_input":"2023-12-05T07:32:07.995059Z","iopub.status.idle":"2023-12-05T07:32:14.231227Z","shell.execute_reply.started":"2023-12-05T07:32:07.995028Z","shell.execute_reply":"2023-12-05T07:32:14.229978Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"for i in range(len(list_essay)):\n    value = apply_was_1(list_essay[i], distribution_r)\n    list_value_3.append(value)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:32:14.233082Z","iopub.execute_input":"2023-12-05T07:32:14.233749Z","iopub.status.idle":"2023-12-05T07:32:20.369263Z","shell.execute_reply.started":"2023-12-05T07:32:14.233715Z","shell.execute_reply":"2023-12-05T07:32:20.368448Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"code","source":"list_value_2[1262]","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:32:20.370587Z","iopub.execute_input":"2023-12-05T07:32:20.371442Z","iopub.status.idle":"2023-12-05T07:32:20.378569Z","shell.execute_reply.started":"2023-12-05T07:32:20.371408Z","shell.execute_reply":"2023-12-05T07:32:20.377120Z"},"trusted":true},"execution_count":142,"outputs":[{"execution_count":142,"output_type":"execute_result","data":{"text/plain":"0.002121154764471281"},"metadata":{}}]},{"cell_type":"code","source":"\n\n# Create a list of tuples containing (index, value)\nindexed_values = list(enumerate(list_value_2))\n\n# Sort the list of tuples based on values\nsorted_values_with_index = sorted(indexed_values, key=lambda x: x[1])\n\n# Calculate the number of values to keep (80%)\nnum_values_to_keep = int(len(sorted_values_with_index) * 0.8)\n\n# Take the top 80% of values and indices\ntop_values_with_index = sorted_values_with_index[-num_values_to_keep:]\n\n# Extract sorted values and indices from the top values with indices\nsorted_values = [value for index, value in top_values_with_index]\nsorted_indices = [index for index, value in top_values_with_index]\n\n# Print the original and sorted values with indices\n#print(\"Original Values:\", values)\n#print(\"Sorted Values (Top 80%):\", sorted_values)\n#print(\"Sorted Indices (Top 80%):\", sorted_indices)\ncount = 0\nindex = [704, 1262]\nfor i in range(2):\n    for j in sorted_indices:\n        if index[i] == j:\n            count = count+1\n            p = index[i]\nif count == 0:\n    print(\"Victory\")\nelse:\n    print(p)\n       ","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:32:20.380300Z","iopub.execute_input":"2023-12-05T07:32:20.380716Z","iopub.status.idle":"2023-12-05T07:32:20.394922Z","shell.execute_reply.started":"2023-12-05T07:32:20.380674Z","shell.execute_reply":"2023-12-05T07:32:20.393449Z"},"trusted":true},"execution_count":143,"outputs":[{"name":"stdout","text":"Victory\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\n# Create a list of tuples containing (index, value)\nindexed_values = list(enumerate(list_value_3))\n\n# Sort the list of tuples based on values\nsorted_values_with_index = sorted(indexed_values, key=lambda x: x[1])\n\n# Calculate the number of values to keep (80%)\nnum_values_to_keep = int(len(sorted_values_with_index) * 0.8)\n\n# Take the top 80% of values and indices\ntop_values_with_index = sorted_values_with_index[-num_values_to_keep:]\n\n# Extract sorted values and indices from the top values with indices\nsorted_values = [value for index, value in top_values_with_index]\nsorted_indices = [index for index, value in top_values_with_index]\n\n# Print the original and sorted values with indices\n#print(\"Original Values:\", values)\n#print(\"Sorted Values (Top 80%):\", sorted_values)\n#print(\"Sorted Indices (Top 80%):\", sorted_indices)\ncount = 0\nindex = [740, 1262]\nfor i in range(2):\n    for j in sorted_indices:\n        if index[i] == j:\n            count = count+1\n            p = index[i]\nif count == 0:\n    print(\"Victory\")\nelse:\n    print(p)\n       ","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:32:20.396585Z","iopub.execute_input":"2023-12-05T07:32:20.397085Z","iopub.status.idle":"2023-12-05T07:32:20.412962Z","shell.execute_reply.started":"2023-12-05T07:32:20.397045Z","shell.execute_reply":"2023-12-05T07:32:20.411565Z"},"trusted":true},"execution_count":144,"outputs":[{"name":"stdout","text":"740\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\n# Create a list of tuples containing (index, value)\nindexed_values = list(enumerate(list_value_1))\n\n# Sort the list of tuples based on values\nsorted_values_with_index = sorted(indexed_values, key=lambda x: x[1])\n\n# Calculate the number of values to keep (80%)\nnum_values_to_keep = int(len(sorted_values_with_index) * 0.8)\n\n# Take the top 80% of values and indices\ntop_values_with_index = sorted_values_with_index[-num_values_to_keep:]\n\n# Extract sorted values and indices from the top values with indices\nsorted_values = [value for index, value in top_values_with_index]\nsorted_indices = [index for index, value in top_values_with_index]\n\n# Print the original and sorted values with indices\n#print(\"Original Values:\", values)\n#print(\"Sorted Values (Top 80%):\", sorted_values)\n#print(\"Sorted Indices (Top 80%):\", sorted_indices)\ncount = 0\nindex = [740, 1262]\nfor i in range(2):\n    for j in sorted_indices:\n        if index[i] == j:\n            count = count+1\n            p = index[i]\nif count == 0:\n    print(\"Victory\")\nelse:\n    print(p)\n       ","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:32:20.415013Z","iopub.execute_input":"2023-12-05T07:32:20.415375Z","iopub.status.idle":"2023-12-05T07:32:20.430372Z","shell.execute_reply.started":"2023-12-05T07:32:20.415344Z","shell.execute_reply":"2023-12-05T07:32:20.429466Z"},"trusted":true},"execution_count":145,"outputs":[{"name":"stdout","text":"Victory\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Here, we take the vector size of word2vec as 100 and take mean as our word2vec representation.\n# **We could study our difference from this in a distribution.**","metadata":{}},{"cell_type":"code","source":"from gensim.models import Word2Vec\nfrom nltk.tokenize import word_tokenize\n\n# Assuming list_essay is a list of sentences\ntokenized_sentences = [word_tokenize(sentence) for sentence in list_essay]\n\n# Define the Word2Vec model\nmodel = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)  # Adjust the parameters as needed\n\n# Training the model\nmodel.train(tokenized_sentences, total_examples=len(tokenized_sentences), epochs=10)\n\ndef mean_vector(words, model):\n    vectors = [model.wv[word] for word in words if word in model.wv]\n    \n    if vectors:\n        return sum(vectors) / len(vectors)\n    else:\n        # Handle the case where none of the words are in the vocabulary\n        return None\n\ndef tokenize_word_vec(essay, model):\n    tokenized_words = word_tokenize(essay)\n    mean_vec = mean_vector(tokenized_words, model)\n\n    if mean_vec is not None:\n        return mean_vec.tolist()  # Convert NumPy array to a Python list\n    else:\n        return None\n\n# Example usage:\nessay = \"Your essay text here.\"\nresult = tokenize_word_vec(essay, model)\n\nif result is not None:\n    print(\"Mean Vector:\", result)\nelse:\n    print(\"None of the words are in the vocabulary.\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:32:20.432156Z","iopub.execute_input":"2023-12-05T07:32:20.432517Z","iopub.status.idle":"2023-12-05T07:32:30.906030Z","shell.execute_reply.started":"2023-12-05T07:32:20.432488Z","shell.execute_reply":"2023-12-05T07:32:30.904950Z"},"trusted":true},"execution_count":146,"outputs":[{"name":"stdout","text":"Mean Vector: [-0.1818123161792755, 0.19892928004264832, -0.0527321919798851, -0.1280875951051712, 0.22281667590141296, 0.3986586630344391, -0.2977621853351593, 0.7825870513916016, 0.22386297583580017, 0.3402906656265259, 0.010022010654211044, -0.5425267815589905, -0.10606946051120758, 0.008115455508232117, -0.22401419281959534, 0.2486446499824524, -0.029243454337120056, -0.22155436873435974, 0.029437944293022156, -0.5022236108779907, -0.05768073350191116, -0.23844251036643982, -0.4593018889427185, -0.46067485213279724, 0.03546224534511566, 0.10084310919046402, 0.13307642936706543, -0.3563069701194763, -0.25381070375442505, -0.04381021112203598, 0.0629967749118805, 0.15012551844120026, -0.11343321949243546, -0.2970835566520691, -0.12153058499097824, 0.015092439949512482, -0.1319258064031601, -0.26318755745887756, -0.028551727533340454, -0.7739123106002808, 0.14890047907829285, -0.4444156885147095, -0.6161304712295532, 0.2778577506542206, 0.17100687325000763, -0.15806074440479279, -0.5030990839004517, 0.4762132167816162, 0.06854245066642761, -0.15378953516483307, -0.12460225075483322, -0.0679316520690918, 0.15492452681064606, 0.3126105070114136, 0.2232397347688675, 0.14207477867603302, -0.09075438976287842, -0.133489191532135, -0.12280691415071487, -0.20841988921165466, 0.3174291253089905, -0.1564309000968933, -0.12568742036819458, 0.00013810396194458008, -0.1165299266576767, 0.14920037984848022, 0.0003645867109298706, 0.5782222747802734, -0.025727342814207077, 0.05321529507637024, 0.71480393409729, 0.3427197337150574, 0.2145557701587677, 0.27223432064056396, 0.2553196847438812, 0.02410256490111351, 0.005065172910690308, -0.5596039295196533, 0.018334824591875076, 0.17745469510555267, 0.1420360505580902, -0.8284651041030884, 0.24952362477779388, 0.08004790544509888, 0.5399703979492188, -0.16398045420646667, -0.09542790800333023, 0.21896588802337646, -0.021975211799144745, 0.48929208517074585, 0.04102861136198044, 0.7474023699760437, 0.5713462829589844, -0.05413498729467392, 0.40450751781463623, 0.6434495449066162, -0.07562100887298584, 0.029504187405109406, 0.0273083858191967, 0.00939933955669403]\n","output_type":"stream"}]},{"cell_type":"code","source":"vec_llm1 = tokenize_word_vec(essay_llm_1, model)\nvec_llm2 = tokenize_word_vec(essay_llm_2, model)\nvec_llm3 = tokenize_word_vec(essay_llm_3, model)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:32:30.907607Z","iopub.execute_input":"2023-12-05T07:32:30.907956Z","iopub.status.idle":"2023-12-05T07:32:30.921198Z","shell.execute_reply.started":"2023-12-05T07:32:30.907925Z","shell.execute_reply":"2023-12-05T07:32:30.920205Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"code","source":"\nlist_value_1 = []\nlist_value_2 = []\nlist_value_3 = []\ndistribution_p = np.array(vec_llm1) / np.sum(vec_llm1)\ndistribution_q = np.array(vec_llm2) / np.sum(vec_llm2)\ndistribution_r = np.array(vec_llm3) / np.sum(vec_llm3)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:32:30.922422Z","iopub.execute_input":"2023-12-05T07:32:30.922793Z","iopub.status.idle":"2023-12-05T07:32:30.931708Z","shell.execute_reply.started":"2023-12-05T07:32:30.922764Z","shell.execute_reply":"2023-12-05T07:32:30.930590Z"},"trusted":true},"execution_count":148,"outputs":[]},{"cell_type":"code","source":"def apply_was(essay_stu_1, dist_p, model):\n    list_1 = []\n    vec_stu = tokenize_word_vec(essay_stu_1, model)\n    dist_a = np.array(vec_stu) / np.sum(vec_stu)\n    wass_val = wasserstein_distance(dist_a, dist_p)\n    list_1.append(wass_val)\n    return wass_val","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:32:30.933210Z","iopub.execute_input":"2023-12-05T07:32:30.934400Z","iopub.status.idle":"2023-12-05T07:32:30.944848Z","shell.execute_reply.started":"2023-12-05T07:32:30.934367Z","shell.execute_reply":"2023-12-05T07:32:30.943746Z"},"trusted":true},"execution_count":149,"outputs":[]},{"cell_type":"code","source":"list_val_mean_1 = []\nlist_val_mean_2 = []\nlist_val_mean_3 = []","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:32:30.946315Z","iopub.execute_input":"2023-12-05T07:32:30.946714Z","iopub.status.idle":"2023-12-05T07:32:30.955466Z","shell.execute_reply.started":"2023-12-05T07:32:30.946686Z","shell.execute_reply":"2023-12-05T07:32:30.954611Z"},"trusted":true},"execution_count":150,"outputs":[]},{"cell_type":"code","source":"for i in range(len(list_essay)):\n    value = apply_was(list_essay[i], distribution_p, model)\n    list_val_mean_1.append(value)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:32:30.956742Z","iopub.execute_input":"2023-12-05T07:32:30.957461Z","iopub.status.idle":"2023-12-05T07:32:37.633895Z","shell.execute_reply.started":"2023-12-05T07:32:30.957409Z","shell.execute_reply":"2023-12-05T07:32:37.632827Z"},"trusted":true},"execution_count":151,"outputs":[]},{"cell_type":"code","source":"for i in range(len(list_essay)):\n    value = apply_was(list_essay[i], distribution_q, model)\n    list_val_mean_2.append(value)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:32:37.635274Z","iopub.execute_input":"2023-12-05T07:32:37.635659Z","iopub.status.idle":"2023-12-05T07:32:44.422938Z","shell.execute_reply.started":"2023-12-05T07:32:37.635628Z","shell.execute_reply":"2023-12-05T07:32:44.421599Z"},"trusted":true},"execution_count":152,"outputs":[]},{"cell_type":"code","source":"for i in range(len(list_essay)):\n    value = apply_was(list_essay[i], distribution_r, model)\n    list_val_mean_3.append(value)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:32:44.424480Z","iopub.execute_input":"2023-12-05T07:32:44.424873Z","iopub.status.idle":"2023-12-05T07:32:51.142252Z","shell.execute_reply.started":"2023-12-05T07:32:44.424843Z","shell.execute_reply":"2023-12-05T07:32:51.141103Z"},"trusted":true},"execution_count":153,"outputs":[]},{"cell_type":"code","source":"list_val_mean_2[1262]","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:32:51.143791Z","iopub.execute_input":"2023-12-05T07:32:51.144151Z","iopub.status.idle":"2023-12-05T07:32:51.151124Z","shell.execute_reply.started":"2023-12-05T07:32:51.144120Z","shell.execute_reply":"2023-12-05T07:32:51.150031Z"},"trusted":true},"execution_count":154,"outputs":[{"execution_count":154,"output_type":"execute_result","data":{"text/plain":"0.13209492422537786"},"metadata":{}}]},{"cell_type":"code","source":"\n\n# Create a list of tuples containing (index, value)\nindexed_values = list(enumerate(list_val_mean_1))\n\n# Sort the list of tuples based on values\nsorted_values_with_index = sorted(indexed_values, key=lambda x: x[1])\n\n# Calculate the number of values to keep (80%)\nnum_values_to_keep = int(len(sorted_values_with_index) * 0.8)\n\n# Take the top 80% of values and indices\ntop_values_with_index = sorted_values_with_index[-num_values_to_keep:]\n\n# Extract sorted values and indices from the top values with indices\nsorted_values = [value for index, value in top_values_with_index]\nsorted_indices = [index for index, value in top_values_with_index]\n\n# Print the original and sorted values with indices\n#print(\"Original Values:\", values)\n#print(\"Sorted Values (Top 80%):\", sorted_values)\n#print(\"Sorted Indices (Top 80%):\", sorted_indices)\ncount = 0\nindex = [740, 1262]\nfor i in range(2):\n    for j in sorted_indices:\n        if index[i] == j:\n            count = count+1\n            p = index[i]\nif count == 0:\n    print(\"Victory\")\nelse:\n    print(p)\n       ","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:32:51.159969Z","iopub.execute_input":"2023-12-05T07:32:51.160881Z","iopub.status.idle":"2023-12-05T07:32:51.174691Z","shell.execute_reply.started":"2023-12-05T07:32:51.160842Z","shell.execute_reply":"2023-12-05T07:32:51.173501Z"},"trusted":true},"execution_count":155,"outputs":[{"name":"stdout","text":"1262\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\n# Create a list of tuples containing (index, value)\nindexed_values = list(enumerate(list_val_mean_2))\n\n# Sort the list of tuples based on values\nsorted_values_with_index = sorted(indexed_values, key=lambda x: x[1])\n\n# Calculate the number of values to keep (80%)\nnum_values_to_keep = int(len(sorted_values_with_index) * 0.8)\n\n# Take the top 80% of values and indices\ntop_values_with_index = sorted_values_with_index[-num_values_to_keep:]\n\n# Extract sorted values and indices from the top values with indices\nsorted_values = [value for index, value in top_values_with_index]\nsorted_indices = [index for index, value in top_values_with_index]\n\n# Print the original and sorted values with indices\n#print(\"Original Values:\", values)\n#print(\"Sorted Values (Top 80%):\", sorted_values)\n#print(\"Sorted Indices (Top 80%):\", sorted_indices)\ncount = 0\nindex = [704, 1262]\nfor i in range(2):\n    for j in sorted_indices:\n        if index[i] == j:\n            count = count+1\n            p = index[i]\nif count == 0:\n    print(\"Victory\")\nelse:\n    print(p)\n       ","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:32:51.176155Z","iopub.execute_input":"2023-12-05T07:32:51.176513Z","iopub.status.idle":"2023-12-05T07:32:51.188782Z","shell.execute_reply.started":"2023-12-05T07:32:51.176483Z","shell.execute_reply":"2023-12-05T07:32:51.187820Z"},"trusted":true},"execution_count":156,"outputs":[{"name":"stdout","text":"1262\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\n# Create a list of tuples containing (index, value)\nindexed_values = list(enumerate(list_val_mean_3))\n\n# Sort the list of tuples based on values\nsorted_values_with_index = sorted(indexed_values, key=lambda x: x[1])\n\n# Calculate the number of values to keep (80%)\nnum_values_to_keep = int(len(sorted_values_with_index) * 0.8)\n\n# Take the top 80% of values and indices\ntop_values_with_index = sorted_values_with_index[-num_values_to_keep:]\n\n# Extract sorted values and indices from the top values with indices\nsorted_values = [value for index, value in top_values_with_index]\nsorted_indices = [index for index, value in top_values_with_index]\n\n# Print the original and sorted values with indices\n#print(\"Original Values:\", values)\n#print(\"Sorted Values (Top 80%):\", sorted_values)\n#print(\"Sorted Indices (Top 80%):\", sorted_indices)\ncount = 0\nindex = [740, 704]\nfor i in range(2):\n    for j in sorted_indices:\n        if index[i] == j:\n            count = count+1\n            p = index[i]\nif count == 0:\n    print(\"Victory\")\nelse:\n    print(p)\n       ","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:32:51.190074Z","iopub.execute_input":"2023-12-05T07:32:51.190821Z","iopub.status.idle":"2023-12-05T07:32:51.209243Z","shell.execute_reply.started":"2023-12-05T07:32:51.190789Z","shell.execute_reply":"2023-12-05T07:32:51.207646Z"},"trusted":true},"execution_count":157,"outputs":[{"name":"stdout","text":"704\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Let's do it for One hot Encoder","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MultiLabelBinarizer","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:32:51.470063Z","iopub.execute_input":"2023-12-05T07:32:51.470454Z","iopub.status.idle":"2023-12-05T07:32:51.475957Z","shell.execute_reply.started":"2023-12-05T07:32:51.470421Z","shell.execute_reply":"2023-12-05T07:32:51.474656Z"},"trusted":true},"execution_count":158,"outputs":[]},{"cell_type":"code","source":"mlb = MultiLabelBinarizer()\none_hot_encoded = mlb.fit_transform(list_essay)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:34:10.820735Z","iopub.execute_input":"2023-12-05T07:34:10.821243Z","iopub.status.idle":"2023-12-05T07:34:11.256207Z","shell.execute_reply.started":"2023-12-05T07:34:10.821202Z","shell.execute_reply":"2023-12-05T07:34:11.254823Z"},"trusted":true},"execution_count":160,"outputs":[]},{"cell_type":"code","source":"vec_llm1 = one_hot_encoded[704]\nvec_llm2 = one_hot_encoded[740]\nvec_llm3 = one_hot_encoded[1262]","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:37:13.951100Z","iopub.execute_input":"2023-12-05T07:37:13.951831Z","iopub.status.idle":"2023-12-05T07:37:13.959105Z","shell.execute_reply.started":"2023-12-05T07:37:13.951775Z","shell.execute_reply":"2023-12-05T07:37:13.957259Z"},"trusted":true},"execution_count":162,"outputs":[]},{"cell_type":"code","source":"list_value_1 = []\nlist_value_2 = []\nlist_value_3 = []\ndistribution_p = np.array(vec_llm1) / np.sum(vec_llm1)\ndistribution_q = np.array(vec_llm2) / np.sum(vec_llm2)\ndistribution_r = np.array(vec_llm3) / np.sum(vec_llm3)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:40:30.920314Z","iopub.execute_input":"2023-12-05T07:40:30.921733Z","iopub.status.idle":"2023-12-05T07:40:30.928817Z","shell.execute_reply.started":"2023-12-05T07:40:30.921681Z","shell.execute_reply":"2023-12-05T07:40:30.927598Z"},"trusted":true},"execution_count":163,"outputs":[]},{"cell_type":"code","source":"def apply_was_hot(essay_stu_1, dist_p):\n    \n    dist_a = np.array(essay_stu_1) / np.sum(essay_stu_1)\n    wass_val = wasserstein_distance(dist_a, dist_p)\n    \n    return wass_val","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:44:54.408937Z","iopub.execute_input":"2023-12-05T07:44:54.409367Z","iopub.status.idle":"2023-12-05T07:44:54.415505Z","shell.execute_reply.started":"2023-12-05T07:44:54.409326Z","shell.execute_reply":"2023-12-05T07:44:54.414459Z"},"trusted":true},"execution_count":170,"outputs":[]},{"cell_type":"code","source":"list_one_hot_1 = []\nlist_one_hot_2 = []\nlist_one_hot_3 = []","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:44:54.996165Z","iopub.execute_input":"2023-12-05T07:44:54.996592Z","iopub.status.idle":"2023-12-05T07:44:55.002850Z","shell.execute_reply.started":"2023-12-05T07:44:54.996558Z","shell.execute_reply":"2023-12-05T07:44:55.001543Z"},"trusted":true},"execution_count":171,"outputs":[]},{"cell_type":"code","source":"for i in range(len(list_essay)):\n    value = apply_was_hot(one_hot_encoded[i], distribution_p)\n    list_one_hot_1.append(value)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:46:30.355744Z","iopub.execute_input":"2023-12-05T07:46:30.356202Z","iopub.status.idle":"2023-12-05T07:46:30.448838Z","shell.execute_reply.started":"2023-12-05T07:46:30.356170Z","shell.execute_reply":"2023-12-05T07:46:30.447590Z"},"trusted":true},"execution_count":173,"outputs":[]},{"cell_type":"code","source":"for i in range(len(list_essay)):\n    value = apply_was_hot(one_hot_encoded[i], distribution_p)\n    list_one_hot_2.append(value)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:46:41.198654Z","iopub.execute_input":"2023-12-05T07:46:41.199082Z","iopub.status.idle":"2023-12-05T07:46:41.291291Z","shell.execute_reply.started":"2023-12-05T07:46:41.199052Z","shell.execute_reply":"2023-12-05T07:46:41.290311Z"},"trusted":true},"execution_count":175,"outputs":[]},{"cell_type":"code","source":"for i in range(len(list_essay)):\n    value = apply_was_hot(one_hot_encoded[i], distribution_p)\n    list_one_hot_3.append(value)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:46:54.977216Z","iopub.execute_input":"2023-12-05T07:46:54.977655Z","iopub.status.idle":"2023-12-05T07:46:55.071006Z","shell.execute_reply.started":"2023-12-05T07:46:54.977623Z","shell.execute_reply":"2023-12-05T07:46:55.069813Z"},"trusted":true},"execution_count":176,"outputs":[]},{"cell_type":"code","source":"\n\n# Create a list of tuples containing (index, value)\nindexed_values = list(enumerate(list_one_hot_1))\n\n# Sort the list of tuples based on values\nsorted_values_with_index = sorted(indexed_values, key=lambda x: x[1])\n\n# Calculate the number of values to keep (80%)\nnum_values_to_keep = int(len(sorted_values_with_index) * 0.8)\n\n# Take the top 80% of values and indices\ntop_values_with_index = sorted_values_with_index[-num_values_to_keep:]\n\n# Extract sorted values and indices from the top values with indices\nsorted_values = [value for index, value in top_values_with_index]\nsorted_indices = [index for index, value in top_values_with_index]\n\n# Print the original and sorted values with indices\n#print(\"Original Values:\", values)\n#print(\"Sorted Values (Top 90%):\", sorted_values)\n#print(\"Sorted Indices (Top 90%):\", sorted_indices)\ncount = 0\nindex = [740, 1262]\nfor i in range(2):\n    for j in sorted_indices:\n        if index[i] == j:\n            count = count+1\n            p = index[i]\nif count == 0:\n    print(\"Victory\")\nelse:\n    print(p)\n       ","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:49:17.522083Z","iopub.execute_input":"2023-12-05T07:49:17.522582Z","iopub.status.idle":"2023-12-05T07:49:17.535213Z","shell.execute_reply.started":"2023-12-05T07:49:17.522546Z","shell.execute_reply":"2023-12-05T07:49:17.533839Z"},"trusted":true},"execution_count":186,"outputs":[{"name":"stdout","text":"740\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\n# Create a list of tuples containing (index, value)\nindexed_values = list(enumerate(list_one_hot_2))\n\n# Sort the list of tuples based on values\nsorted_values_with_index = sorted(indexed_values, key=lambda x: x[1])\n\n# Calculate the number of values to keep (80%)\nnum_values_to_keep = int(len(sorted_values_with_index) * 0.9)\n\n# Take the top 80% of values and indices\ntop_values_with_index = sorted_values_with_index[-num_values_to_keep:]\n\n# Extract sorted values and indices from the top values with indices\nsorted_values = [value for index, value in top_values_with_index]\nsorted_indices = [index for index, value in top_values_with_index]\n\n# Print the original and sorted values with indices\n#print(\"Original Values:\", values)\n#print(\"Sorted Values (Top 90%):\", sorted_values)\n#print(\"Sorted Indices (Top 90%):\", sorted_indices)\ncount = 0\nindex = [704, 1262]\nfor i in range(2):\n    for j in sorted_indices:\n        if index[i] == j:\n            count = count+1\n            p = index[i]\nif count == 0:\n    print(\"Victory\")\nelse:\n    print(p)\n       ","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:49:18.676680Z","iopub.execute_input":"2023-12-05T07:49:18.677164Z","iopub.status.idle":"2023-12-05T07:49:18.690904Z","shell.execute_reply.started":"2023-12-05T07:49:18.677124Z","shell.execute_reply":"2023-12-05T07:49:18.689308Z"},"trusted":true},"execution_count":187,"outputs":[{"name":"stdout","text":"Victory\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\n# Create a list of tuples containing (index, value)\nindexed_values = list(enumerate(list_one_hot_3))\n\n# Sort the list of tuples based on values\nsorted_values_with_index = sorted(indexed_values, key=lambda x: x[1])\n\n# Calculate the number of values to keep (80%)\nnum_values_to_keep = int(len(sorted_values_with_index) * 0.9)\n\n# Take the top 80% of values and indices\ntop_values_with_index = sorted_values_with_index[-num_values_to_keep:]\n\n# Extract sorted values and indices from the top values with indices\nsorted_values = [value for index, value in top_values_with_index]\nsorted_indices = [index for index, value in top_values_with_index]\n\n# Print the original and sorted values with indices\n#print(\"Original Values:\", values)\n#print(\"Sorted Values (Top 90%):\", sorted_values)\n#print(\"Sorted Indices (Top 90%):\", sorted_indices)\ncount = 0\nindex = [740, 704]\nfor i in range(2):\n    for j in sorted_indices:\n        if index[i] == j:\n            count = count+1\n            p = index[i]\nif count == 0:\n    print(\"Victory\")\nelse:\n    print(p)\n       ","metadata":{"execution":{"iopub.status.busy":"2023-12-05T07:49:21.037047Z","iopub.execute_input":"2023-12-05T07:49:21.037507Z","iopub.status.idle":"2023-12-05T07:49:21.051309Z","shell.execute_reply.started":"2023-12-05T07:49:21.037473Z","shell.execute_reply":"2023-12-05T07:49:21.049545Z"},"trusted":true},"execution_count":188,"outputs":[{"name":"stdout","text":"740\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}